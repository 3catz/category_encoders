

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>James-Stein Encoder &mdash; Category Encoders latest documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Category Encoders
          

          
          </a>

          
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="backward_difference.html">Backward Difference Coding</a></li>
<li class="toctree-l1"><a class="reference internal" href="basen.html">BaseN</a></li>
<li class="toctree-l1"><a class="reference internal" href="binary.html">Binary</a></li>
<li class="toctree-l1"><a class="reference internal" href="catboost.html">CatBoost Encoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="hashing.html">Hashing</a></li>
<li class="toctree-l1"><a class="reference internal" href="helmert.html">Helmert Coding</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">James-Stein Encoder</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="leaveoneout.html">Leave One Out</a></li>
<li class="toctree-l1"><a class="reference internal" href="mestimate.html">M-estimate</a></li>
<li class="toctree-l1"><a class="reference internal" href="onehot.html">One Hot</a></li>
<li class="toctree-l1"><a class="reference internal" href="ordinal.html">Ordinal</a></li>
<li class="toctree-l1"><a class="reference internal" href="polynomial.html">Polynomial Coding</a></li>
<li class="toctree-l1"><a class="reference internal" href="sum.html">Sum Coding</a></li>
<li class="toctree-l1"><a class="reference internal" href="targetencoder.html">Target Encoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="woe.html">Weight of Evidence</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Category Encoders</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>James-Stein Encoder</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/jamesstein.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="james-stein-encoder">
<h1>James-Stein Encoder<a class="headerlink" href="#james-stein-encoder" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="category_encoders.james_stein.JamesSteinEncoder">
<em class="property">class </em><code class="descclassname">category_encoders.james_stein.</code><code class="descname">JamesSteinEncoder</code><span class="sig-paren">(</span><em>verbose=0</em>, <em>cols=None</em>, <em>drop_invariant=False</em>, <em>return_df=True</em>, <em>handle_unknown='value'</em>, <em>handle_missing='value'</em>, <em>model='independent'</em>, <em>random_state=None</em>, <em>randomized=False</em>, <em>sigma=0.05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/category_encoders/james_stein.html#JamesSteinEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#category_encoders.james_stein.JamesSteinEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>James-Stein estimator.</p>
<p>For feature value <cite>i</cite>, James-Stein estimator returns a weighted average of:</p>
<blockquote>
<div><ol class="arabic simple">
<li>The mean target value for the observed feature value <cite>i</cite>.</li>
<li>The mean target value (regardless of the feature value).</li>
</ol>
</div></blockquote>
<p>This can be written as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">JS_i</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">B</span><span class="p">)</span><span class="o">*</span><span class="n">mean</span><span class="p">(</span><span class="n">y_i</span><span class="p">)</span> <span class="o">+</span> <span class="n">B</span><span class="o">*</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>The question is, what should be the weight <cite>B</cite>?
If we put too much weight on the conditional mean value, we will overfit.
If we put too much weight on the global mean, we will underfit.
The canonical solution in machine learning is to perform cross-validation.
However, Charles Stein came with a closed-form solution to the problem.
The intuition is: If the estimate of <cite>mean(y_i)</cite> is unreliable (<cite>y_i</cite> has high variance),
we should put more weight on <cite>mean(y)</cite>. Stein put it into an equation as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">B</span> <span class="o">=</span> <span class="n">var</span><span class="p">(</span><span class="n">y_i</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">var</span><span class="p">(</span><span class="n">y_i</span><span class="p">)</span><span class="o">+</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p>The only remaining issue is that we do not know <cite>var(y)</cite>, let alone <cite>var(y_i)</cite>.
Hence, we have to estimate the variances. But how can we reliably estimate the
variances, when we already struggle with the estimation of the mean values?!
There are multiple solutions:</p>
<blockquote>
<div><p>1. If we have the same count of observations for each feature value <cite>i</cite> and all
<cite>y_i</cite> are close to each other, we can pretend that all <cite>var(y_i)</cite> are identical.
This is called a pooled model.
2. If the observation counts are not equal, it makes sense to replace the variances
with squared standard errors, which penalize small observation counts:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SE</span><span class="o">^</span><span class="mi">2</span> <span class="o">=</span> <span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">count</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>This is called an independent model.</p>
</div></blockquote>
<p>James-Stein estimator has, however, one practical limitation - it was defined
only for normal distributions. If you want to apply it for binary classification,
which allows only values {0, 1}, it is better to first convert the mean target value
from the bound interval &lt;0,1&gt; into an unbounded interval by replacing mean(y)
with log-odds ratio:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">log</span><span class="o">-</span><span class="n">odds_ratio_i</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">y_i</span><span class="p">)</span><span class="o">/</span><span class="n">mean</span><span class="p">(</span><span class="n">y_not_i</span><span class="p">))</span>
</pre></div>
</div>
<p>This is called binary model. The estimation of parameters of this model is, however,
tricky and sometimes it fails fatally. In these situations, it is better to use beta
model, which generally delivers slightly worse accuracy than binary model but does
not suffer from fatal failures.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><dl class="first last docutils">
<dt><strong>verbose: int</strong></dt>
<dd><p class="first last">integer indicating verbosity of the output. 0 for none.</p>
</dd>
<dt><strong>cols: list</strong></dt>
<dd><p class="first last">a list of columns to encode, if None, all string columns will be encoded.</p>
</dd>
<dt><strong>drop_invariant: bool</strong></dt>
<dd><p class="first last">boolean for whether or not to drop encoded columns with 0 variance.</p>
</dd>
<dt><strong>return_df: bool</strong></dt>
<dd><p class="first last">boolean for whether to return a pandas DataFrame from transform (otherwise it will be a numpy array).</p>
</dd>
<dt><strong>handle_missing: str</strong></dt>
<dd><p class="first last">options are ‘return_nan’, ‘error’ and ‘value’, defaults to ‘value’, which returns the prior probability.</p>
</dd>
<dt><strong>handle_unknown: str</strong></dt>
<dd><p class="first last">options are ‘return_nan’, ‘error’ and ‘value’, defaults to ‘value’, which returns the prior probability.</p>
</dd>
<dt><strong>model: str</strong></dt>
<dd><p class="first last">options are ‘pooled’, ‘beta’, ‘binary’ and ‘independent’, defaults to ‘independent’.</p>
</dd>
<dt><strong>randomized: bool,</strong></dt>
<dd><p class="first last">adds normal (Gaussian) distribution noise into training data in order to decrease overfitting (testing data are untouched).</p>
</dd>
<dt><strong>sigma: float</strong></dt>
<dd><p class="first last">standard deviation (spread or “width”) of the normal distribution.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="r50705e07df56-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[R50705e07df56-1]</td><td>Parametric empirical Bayes inference: Theory and applications, equations 1.19 &amp; 1.20, from</td></tr>
</tbody>
</table>
<p><a class="reference external" href="https://www.jstor.org/stable/2287098">https://www.jstor.org/stable/2287098</a></p>
<table class="docutils citation" frame="void" id="r50705e07df56-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[R50705e07df56-2]</td><td>Empirical Bayes for multiple sample sizes, from</td></tr>
</tbody>
</table>
<p><a class="reference external" href="http://chris-said.io/2017/05/03/empirical-bayes-for-multiple-sample-sizes/">http://chris-said.io/2017/05/03/empirical-bayes-for-multiple-sample-sizes/</a></p>
<table class="docutils citation" frame="void" id="r50705e07df56-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[R50705e07df56-3]</td><td>Shrinkage Estimation of Log-odds Ratios for Comparing Mobility Tables, from</td></tr>
</tbody>
</table>
<p><a class="reference external" href="https://journals.sagepub.com/doi/abs/10.1177/0081175015570097">https://journals.sagepub.com/doi/abs/10.1177/0081175015570097</a></p>
<table class="docutils citation" frame="void" id="r50705e07df56-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[R50705e07df56-4]</td><td>Stein’s paradox and group rationality, from</td></tr>
</tbody>
</table>
<p><a class="reference external" href="http://www.philos.rug.nl/~romeyn/presentation/">http://www.philos.rug.nl/~romeyn/presentation/</a><a href="#id6"><span class="problematic" id="id7">2017_romeijn_</span></a>-_Paris_Stein.pdf</p>
<table class="docutils citation" frame="void" id="r50705e07df56-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[R50705e07df56-5]</td><td>Stein’s Paradox in Statistics, from</td></tr>
</tbody>
</table>
<p><a class="reference external" href="http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf">http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf</a></p>
<p class="rubric">Methods</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#category_encoders.james_stein.JamesSteinEncoder.fit" title="category_encoders.james_stein.JamesSteinEncoder.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(self,&nbsp;X,&nbsp;y,&nbsp;\*\*kwargs)</td>
<td>Fit encoder according to X and binary y.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#category_encoders.james_stein.JamesSteinEncoder.fit_transform" title="category_encoders.james_stein.JamesSteinEncoder.fit_transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code></a>(self,&nbsp;X[,&nbsp;y])</td>
<td>Encoders that utilize the target must make sure that the training data are transformed with:</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#category_encoders.james_stein.JamesSteinEncoder.get_feature_names" title="category_encoders.james_stein.JamesSteinEncoder.get_feature_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_feature_names</span></code></a>(self)</td>
<td>Returns the names of all transformed / added columns.</td>
</tr>
<tr class="row-even"><td><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code>(self[,&nbsp;deep])</td>
<td>Get parameters for this estimator.</td>
</tr>
<tr class="row-odd"><td><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code>(self,&nbsp;\*\*params)</td>
<td>Set the parameters of this estimator.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#category_encoders.james_stein.JamesSteinEncoder.transform" title="category_encoders.james_stein.JamesSteinEncoder.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(self,&nbsp;X[,&nbsp;y,&nbsp;override_return_df])</td>
<td>Perform the transformation to new categorical data.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="category_encoders.james_stein.JamesSteinEncoder.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>y</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/category_encoders/james_stein.html#JamesSteinEncoder.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#category_encoders.james_stein.JamesSteinEncoder.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit encoder according to X and binary y.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><dl class="first docutils">
<dt><strong>X</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_samples, n_features]</span></dt>
<dd><p class="first last">Training vectors, where n_samples is the number of samples
and n_features is the number of features.</p>
</dd>
<dt><strong>y</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_samples]</span></dt>
<dd><p class="first last">Binary target values.</p>
</dd>
</dl>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><dl class="first last docutils">
<dt><strong>self</strong> <span class="classifier-delimiter">:</span> <span class="classifier">encoder</span></dt>
<dd><p class="first last">Returns self.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="category_encoders.james_stein.JamesSteinEncoder.fit_transform">
<code class="descname">fit_transform</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>y=None</em>, <em>**fit_params</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/category_encoders/james_stein.html#JamesSteinEncoder.fit_transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#category_encoders.james_stein.JamesSteinEncoder.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>Encoders that utilize the target must make sure that the training data are transformed with:</dt>
<dd>transform(X, y)</dd>
<dt>and not with:</dt>
<dd>transform(X)</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="category_encoders.james_stein.JamesSteinEncoder.get_feature_names">
<code class="descname">get_feature_names</code><span class="sig-paren">(</span><em>self</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/category_encoders/james_stein.html#JamesSteinEncoder.get_feature_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#category_encoders.james_stein.JamesSteinEncoder.get_feature_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the names of all transformed / added columns.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><dl class="first last docutils">
<dt>feature_names: list</dt>
<dd><p class="first last">A list with all feature names transformed or added.
Note: potentially dropped features are not included!</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="category_encoders.james_stein.JamesSteinEncoder.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>y=None</em>, <em>override_return_df=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/category_encoders/james_stein.html#JamesSteinEncoder.transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#category_encoders.james_stein.JamesSteinEncoder.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform the transformation to new categorical data. When the data are used for model training,
it is important to also pass the target in order to apply leave one out.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><dl class="first docutils">
<dt><strong>X</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_samples, n_features]</span></dt>
<dd></dd>
<dt><strong>y</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_samples] when transform by leave one out</span></dt>
<dd><p class="first last">None, when transform without target information (such as transform test set)</p>
</dd>
</dl>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><dl class="first last docutils">
<dt><strong>p</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_numeric + N]</span></dt>
<dd><p class="first last">Transformed values with encoding applied.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Will McGinnis

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>